# -*- coding: utf-8 -*-
"""Metricas_de_Qualidade

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrs0EyG2iX0Pbjiubm2TLrlmulGHl0Yr
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, median_absolute_error
from sklearn.model_selection import train_test_split

#carregando o DataSet
x, y = datasets.load_diabetes(return_X_y=True)

#Separamos uma feature para usar 
x = x[:, np.newaxis, 2]

#Separe os dados para testar
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.20, random_state=42)

#intancie 
regr = linear_model.LinearRegression()

#treine o modelo 
regr.fit(x_train, y_train)

#teste o modelo treinado
y_pred= regr.predict(x_test)

#metricas de qualidade
print ('Erro médio quadratico: %.2f' % mean_squared_error(y_test, y_pred))

print('Erro mediano absoluto: %.2f' % median_absolute_error(y_test, y_pred))

#Grafico
plt.scatter(x_test, y_test, color='red')
plt.plot(x_test, y_pred, color='yellow', linewidth=3)
plt.xticks(())
plt.yticks(())
plt.show

"""***Metricas de Qualidade KNN e Ramdow Forest negrito***"""

#carregando o DataSet
from sklearn.datasets import load_wine
wine = load_wine()

#exemplos de aceeso aos dados 
x = wine.data[:,:]#feature de cada elemento
y = wine.target #Classes de cada elemento

#É preciso treinar o classificador, e testar o seu desempenho com dados novos 
#Aqui, dividimos os dados em treino e teste, para podermos testar nosso desempenho depois

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42)
#o uso desta função ajuda mais não é obrigatorio, você pode dividir os seus dados manualmente.

#Carregando e treinando os classificadores 
# Random Forest 
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(x_train, y_train)
y_pred = rfc.predict(x_test)

#Metricas do Random Forest

from sklearn.metrics import  accuracy_score, recall_score, precision_score

rfc_acc = round(accuracy_score(y_test,y_pred), 6) #round é para arrendondar 
rcf_recall = round(recall_score(y_test, y_pred, average='weighted'),6)
rfc_precision = round(precision_score(y_test, y_pred, average='weighted'), 6)

#Knn
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)

#metricas do Knn 
knn_acc = round(accuracy_score(y_test,y_pred), 6) #round é para arrendondar 
knn_recall = round(recall_score(y_test, y_pred, average='weighted'),6)
knn_precision = round(precision_score(y_test, y_pred, average='weighted'), 6)

#Comparação 
print("Knn vs Random Forest\n")
print("Classes: {0}\n".format(wine.target_names))
print("Acuracia:{0} vs {1}".format(knn_acc, rfc_acc))
print("Recall: {0} vs {1}".format(knn_recall, rcf_recall))
print("Precisão: {0} vs {1}".format(knn_precision, rfc_precision))

#Na validação Cruzada 
from sklearn.model_selection import cross_val_score
cv_rfc = cross_val_score(rfc, x, y)
cv_knn = cross_val_score(knn, x, y)
print("\nValidação cruzada: {0} vs {1}".format(cv_knn, cv_rfc))

sum_cv_rfc = 0 
for cv_score in cv_rfc:
  sum_cv_rfc += cv_score 

print("\nResultado Random Forest {0}".format(sum_cv_rfc/5))

sum_cv_knn = 0 
for cv_score in cv_knn:
  sum_cv_knn += cv_score 

print("\nResultado KNN {0}".format(sum_cv_knn/5))

"""# ***Buscando Hiper Parametros ***"""

from sklearn.model_selection import GridSearchCV

#RFC
parameters = {'min_samples_split':(2,6)}
rfc_hps = GridSearchCV(rfc, parameters)
rfc_hps.fit(x,y)
print ("Melhor valor para min_samples_split: {0}".format(rfc_hps.best_params_['min_samples_split']))

#Knn
parameters = {'n_neighbors':(1,20)}
knn_hps = GridSearchCV(knn, parameters)
knn_hps.fit(x,y)
knn_hps.best_params_['n_neighbors']
print ("Melhor valor para n_neighbors: {0}".format(knn_hps.best_params_['n_neighbors']))



